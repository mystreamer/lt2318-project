
@article{navigli_biases_2023,
	title = {Biases in {Large} {Language} {Models}: {Origins}, {Inventory}, and {Discussion}},
	volume = {15},
	issn = {1936-1955, 1936-1963},
	shorttitle = {Biases in {Large} {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3597307},
	doi = {10.1145/3597307},
	abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
	language = {en},
	number = {2},
	urldate = {2024-12-26},
	journal = {Journal of Data and Information Quality},
	author = {Navigli, Roberto and Conia, Simone and Ross, Björn},
	month = jun,
	year = {2023},
	pages = {1--21},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machines learn what people know implicitly
            
              AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan
              et al.
              now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior.
            
            
              Science
              , this issue p.
              183
              ; see also p.
              133
            
          , 
            Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias.
          , 
            Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	language = {en},
	number = {6334},
	urldate = {2024-12-26},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	pages = {183--186},
}

@inproceedings{blodgett_language_2020,
	address = {Online},
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of “{Bias}” in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé Iii, Hal and Wallach, Hanna},
	year = {2020},
	pages = {5454--5476},
}

@inproceedings{nangia_crows-pairs_2020,
	address = {Online},
	title = {{CrowS}-{Pairs}: {A} {Challenge} {Dataset} for {Measuring} {Social} {Biases} in {Masked} {Language} {Models}},
	shorttitle = {{CrowS}-{Pairs}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.154},
	doi = {10.18653/v1/2020.emnlp-main.154},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	year = {2020},
	pages = {1953--1967},
}

@misc{damodaran_parrot_2021,
	title = {Parrot: {Paraphrase} generation for {NLU}.},
	author = {Damodaran, Prithiviraj},
	year = {2021},
	note = {Version Number: v1.0},
}

@inproceedings{dev_measures_2022,
	address = {Online only},
	title = {On {Measures} of {Biases} and {Harms} in {NLP}},
	url = {https://aclanthology.org/2022.findings-aacl.24},
	doi = {10.18653/v1/2022.findings-aacl.24},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {AACL}-{IJCNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Dev, Sunipa and Sheng, Emily and Zhao, Jieyu and Amstutz, Aubrie and Sun, Jiao and Hou, Yu and Sanseverino, Mattie and Kim, Jiin and Nishi, Akihiro and Peng, Nanyun and Chang, Kai-Wei},
	year = {2022},
	pages = {246--267},
}

@inproceedings{raj_biasdora_2024,
	address = {Miami, Florida, USA},
	title = {{BiasDora}: {Exploring} {Hidden} {Biased} {Associations} in {Vision}-{Language} {Models}},
	shorttitle = {{BiasDora}},
	url = {https://aclanthology.org/2024.findings-emnlp.611},
	doi = {10.18653/v1/2024.findings-emnlp.611},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Raj, Chahat and Mukherjee, Anjishnu and Caliskan, Aylin and Anastasopoulos, Antonios and Zhu, Ziwei},
	year = {2024},
	pages = {10439--10455},
}

@misc{zheng_large_2023,
	title = {Large {Language} {Models} {Are} {Not} {Robust} {Multiple} {Choice} {Selectors}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2309.03882},
	doi = {10.48550/ARXIV.2309.03882},
	abstract = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.},
	urldate = {2024-12-26},
	publisher = {arXiv},
	author = {Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
	year = {2023},
	note = {Version Number: 4},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{nadeem_stereoset_2021,
	address = {Online},
	title = {{StereoSet}: {Measuring} stereotypical bias in pretrained language models},
	shorttitle = {{StereoSet}},
	url = {https://aclanthology.org/2021.acl-long.416},
	doi = {10.18653/v1/2021.acl-long.416},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Nadeem, Moin and Bethke, Anna and Reddy, Siva},
	year = {2021},
	pages = {5356--5371},
}

@inproceedings{srinivasan_worst_2022,
	address = {Seattle, Washington},
	title = {Worst of {Both} {Worlds}: {Biases} {Compound} in {Pre}-trained {Vision}-and-{Language} {Models}},
	shorttitle = {Worst of {Both} {Worlds}},
	url = {https://aclanthology.org/2022.gebnlp-1.10},
	doi = {10.18653/v1/2022.gebnlp-1.10},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of the 4th {Workshop} on {Gender} {Bias} in {Natural} {Language} {Processing} ({GeBNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Srinivasan, Tejas and Bisk, Yonatan},
	year = {2022},
	pages = {77--85},
}

@inproceedings{ruggeri_multi-dimensional_2023,
	address = {Toronto, Canada},
	title = {A {Multi}-dimensional study on {Bias} in {Vision}-{Language} models},
	url = {https://aclanthology.org/2023.findings-acl.403},
	doi = {10.18653/v1/2023.findings-acl.403},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Ruggeri, Gabriele and Nozza, Debora},
	year = {2023},
	pages = {6445--6455},
}

@inproceedings{zhou_vlstereoset_2022,
	address = {Online only},
	title = {{VLStereoSet}: {A} {Study} of {Stereotypical} {Bias} in {Pre}-trained {Vision}-{Language} {Models}},
	shorttitle = {{VLStereoSet}},
	url = {https://aclanthology.org/2022.aacl-main.40},
	doi = {10.18653/v1/2022.aacl-main.40},
	language = {en},
	urldate = {2024-12-26},
	booktitle = {Proceedings of the 2nd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} and the 12th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Kankan and Lai, Eason and Jiang, Jing},
	year = {2022},
	pages = {527--538},
}
