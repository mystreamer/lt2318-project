{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ReplicationExperiments\n",
    "\n",
    "This notebooks aims to replicate some results of the paper **VLStereoSet: A Study of Stereotypical Bias in Pre-trained Vision-Language Models** by Zhou et al. (2022).\n",
    "\n",
    "+ For evaluation we use:\n",
    "  + `openai/clip-vit-large-patch14`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Imports\n",
    "import os, sys, json\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "import requests\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dfcd1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5442bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils.utils as utils\n",
    "reload(utils)\n",
    "from utils.utils import \\\n",
    "    calculate_vlrs, \\\n",
    "    calculate_vlbs, \\\n",
    "    calculate_ivlas, \\\n",
    "    read_jsonl, \\\n",
    "    save_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some relevant folders for data persistence\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/K-Square-00/VLStereo/refs/heads/main/data/VLStereoSet.csv\"\n",
    "RESULTS_DIR = \"./results\"\n",
    "MODEL = \"openai/clip-vit-large-patch14\"\n",
    "DATASET_TO_SAVE_FILENAME = f\"{ RESULTS_DIR }/res_{ MODEL.replace('/', '_') }.jsonl\"\n",
    "DEBUG = False\n",
    "RANDOM_SEED = 41\n",
    "DEVICE = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20dca429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dir\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf55fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as pandas dataframe\n",
    "df = pd.read_csv(f\"data/{ DATASET_URL.split('/')[-1] }\")\n",
    "df = df.rename(columns={\"Imaeg URL\": \"image_url\"}).drop(columns=[\"Unnamed: 8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ab33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d4b72dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "import random\n",
    "from io import BytesIO\n",
    "\n",
    "def get_image_data(url):\n",
    "    sleep(random.randint(1, 3))\n",
    "    try:\n",
    "        response = requests.get(url, timeout=20) \n",
    "    except requests.exceptions.Timeout:\n",
    "        raise Exception(\"Timeout error\")\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error: { response.status_code }\")\n",
    "    if \"image\" not in response.headers['Content-Type']:\n",
    "        raise Exception(f\"Error: { response.headers['Content-Type'] }\")\n",
    "    return Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ee2e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load models\n",
    "model = CLIPModel.from_pretrained(MODEL).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee872711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set empty samples list\n",
    "samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660ad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "START_WHERE_LEFT_OFF = True\n",
    "\n",
    "if START_WHERE_LEFT_OFF:\n",
    "    print(\"Taking off from where we left off\")\n",
    "    if Path(DATASET_TO_SAVE_FILENAME).exists():\n",
    "        samples = read_jsonl(DATASET_TO_SAVE_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "non_available_images = 0\n",
    "skip_until = 0\n",
    "\n",
    "# Process data\n",
    "for index, row in tqdm(islice(df.iterrows(), 3 if DEBUG else None), total=3 if DEBUG else len(df)):\n",
    "\n",
    "    if index < len(samples):\n",
    "        print(\"Skipping existing\")\n",
    "        skip_until = index + 1\n",
    "        continue\n",
    "\n",
    "    # Get three options\n",
    "    stereotypical_option = row[\"stereotype\"]\n",
    "    non_stereotypical_option = row[\"anti-stereotype\"]\n",
    "    neutral_option = row[\"unrelated\"]\n",
    "\n",
    "    # Get encoded image\n",
    "    try:\n",
    "        img = get_image_data(row[\"image_url\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: { e }\")\n",
    "        samples.append({ \n",
    "            **row.to_dict(),\n",
    "            \"response\": \"Image URL not available\",\n",
    "            \"order\": pd.NA\n",
    "        }) # insert dummy value\n",
    "        continue\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[stereotypical_option, non_stereotypical_option, neutral_option], \n",
    "        images=img, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True)\n",
    "    outputs = model(**inputs.to(DEVICE))\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    choice = np.argmax(probs.cpu().detach().numpy())\n",
    "    samples.append({ \n",
    "        **row.to_dict(),\n",
    "        \"response\": [\"a\", \"b\", \"c\"][choice], # encode answer as letter\n",
    "        \"order\": [0, 1, 2] # order is not shuffled\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0af53550",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl(samples, DATASET_TO_SAVE_FILENAME, skip_until=skip_until)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9aa80",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2daba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_samples = read_jsonl(DATASET_TO_SAVE_FILENAME)\n",
    "\n",
    "vlrs, vlbs = calculate_vlrs(processed_samples), calculate_vlbs(processed_samples)\n",
    "print(f\"VLRS: { vlrs }\")\n",
    "print(f\"VLBS: { vlbs }\")\n",
    "\n",
    "ivlas = calculate_ivlas(vlrs[0], vlbs[0])\n",
    "print(f\"IVLAS: { ivlas }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
